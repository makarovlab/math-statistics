{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Rule-based matching\n",
    "\n",
    "In this lesson, we'll take a look at spaCy's matcher, which lets you write rules to find words and phrases in text."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Why not just regular expressions?\n",
    "\n",
    "- Match on Doc objects, not just strings\n",
    "- Match on tokens and token attributes\n",
    "- Use a model's predictions\n",
    "- Example: \"duck\" (verb) vs. \"duck\" (noun)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compared to regular expressions, the matcher works with Doc and Token objects instead of only strings.\n",
    "\n",
    "It's also more flexible: you can search for texts but also other lexical attributes.\n",
    "\n",
    "You can even write rules that use a model's predictions.\n",
    "\n",
    "For example, find the word \"duck\" only if it's a verb, not a noun."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Match patterns\n",
    "\n",
    "Match patterns are lists of dictionaries. Each dictionary describes one token. The keys are the names of token attributes, mapped to their expected values.\n",
    "\n",
    "In this example, we're looking for two tokens with the text \"iPhone\" and \"X\".\n",
    "\n",
    "We can also match on other token attributes. Here, we're looking for two tokens whose lowercase forms equal \"iphone\" and \"x\".\n",
    "\n",
    "We can even write patterns using attributes predicted by a model. Here, we're matching a token with the lemma \"buy\", plus a noun. The lemma is the base form, so this pattern would match phrases like \"buying milk\" or \"bought flowers\".\n",
    "\n",
    "\n",
    "- Lists of dictionaries, one per token\n",
    "- Match exact token texts: ```[{\"TEXT\": \"iPhone\"}, {\"TEXT\": \"X\"}]```\n",
    "\n",
    "- Match lexical attributes: ```[{\"LOWER\": \"iphone\"}, {\"LOWER\": \"x\"}]```\n",
    "\n",
    "- Match any token attributes: ```[{\"LEMMA\": \"buy\"}, {\"POS\": \"NOUN\"}]```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Using the Matcher"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. To use a pattern, we first import the matcher from spacy.matcher.\n",
    "\n",
    "2. We also load a pipeline and create the nlp object.\n",
    "\n",
    "3. The matcher is initialized with the shared vocabulary, nlp.vocab. You'll learn more about this later – for now, just remember to always pass it in.\n",
    "\n",
    "4. The matcher.add method lets you add a pattern. The first argument is a unique ID to identify which pattern was matched. The second argument is a list of patterns.\n",
    "\n",
    "5. To match the pattern on a text, we can call the matcher on any doc.\n",
    "\n",
    "6. This will return the matches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/max/.local/lib/python3.10/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "2023-07-22 23:08:32.721346: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F AVX512_VNNI FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-07-22 23:08:32.897836: I tensorflow/core/util/port.cc:104] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2023-07-22 23:08:33.487555: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory\n",
      "2023-07-22 23:08:33.487610: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory\n",
      "2023-07-22 23:08:33.487615: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n",
      "2023-07-22 23:08:34.495720: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory\n",
      "2023-07-22 23:08:34.495754: W tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:265] failed call to cuInit: UNKNOWN ERROR (303)\n",
      "2023-07-22 23:08:34.495769: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (max-laptop): /proc/driver/nvidia/version does not exist\n"
     ]
    }
   ],
   "source": [
    "import spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy.matcher import Matcher"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "matcher = Matcher(nlp.vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "pattern = [{\"TEXT\": \"iPhone\"}, {\"TEXT\": \"X\"}]\n",
    "matcher.add(\"IPHONE_PATTERN\", [pattern])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = nlp(\"Upcoming iPhone X release date leaked\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "matches = matcher(doc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When you call the matcher on a doc, it returns a list of tuples.\n",
    "\n",
    "Each tuple consists of three values: the match ID, the start index and the end index of the matched span.\n",
    "\n",
    "This means we can iterate over the matches and create a Span object: a slice of the doc at the start and end index.\n",
    "\n",
    "- match_id: hash value of the pattern name\n",
    "- start: start index of matched span\n",
    "- end: end index of matched span"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iPhone X\n"
     ]
    }
   ],
   "source": [
    "for match_id, start, end in matches:\n",
    "    matched_span = doc[start:end]\n",
    "    print(matched_span.text) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Matching lexical attributes\n",
    "\n",
    "Here's an example of a more complex pattern using lexical attributes.\n",
    "\n",
    "We're looking for five tokens:\n",
    "\n",
    "- A token consisting of only digits.\n",
    "\n",
    "- Three case-insensitive tokens for \"fifa\", \"world\" and \"cup\".\n",
    "\n",
    "- And a token that consists of punctuation.\n",
    "\n",
    "The pattern matches the tokens \"2018 FIFA World Cup:\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "pattern = [\n",
    "    {\"IS_DIGIT\": True},\n",
    "    {\"LOWER\": \"fifa\"},\n",
    "    {\"LOWER\": \"world\"},\n",
    "    {\"LOWER\": \"cup\"},\n",
    "    {\"IS_PUNCT\": True}\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "matcher.add(\"FIFA_WOLRD_CUP\", [pattern])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = nlp(\"2018 FIFA World Cup: France won!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "matches = matcher(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018 FIFA World Cup:\n"
     ]
    }
   ],
   "source": [
    "for match_id, start, end in matches:\n",
    "    matched_span = doc[start:end]\n",
    "    print(matched_span.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Matching other token attributes\n",
    "\n",
    "In this example, we're looking for two tokens:\n",
    "\n",
    "- A verb with the lemma \"love\", followed by a noun.\n",
    "\n",
    "- This pattern will match \"loved dogs\" and \"love cats\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "pattern = [\n",
    "    {\"LEMMA\": \"love\", \"POS\": \"VERB\"},\n",
    "    {\"POS\": \"NOUN\"}\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "matcher.add(\"LOVE_ANYBODY\", [pattern])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = nlp(\"I loved dogs but now I love cats more.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "matches = matcher(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loved dogs\n",
      "love cats\n"
     ]
    }
   ],
   "source": [
    "for match_id, start, end in matches:\n",
    "    matched_span = doc[start:end]\n",
    "    print(matched_span.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Using operators and quantifiers\n",
    "\n",
    "Operators and quantifiers let you define how often a token should be matched. They can be added using the \"OP\" key.\n",
    "\n",
    "Here, the \"?\" operator makes the determiner token optional, so it will match a token with the lemma \"buy\", an optional article and a noun."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "pattern = [\n",
    "    {\"LEMMA\": \"buy\"},\n",
    "    {\"POS\": \"DET\", \"OP\": \"?\"},\n",
    "    {\"POS\": \"NOUN\"}\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = nlp(\"I bought a smartphone. Now I'm buying apps.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "matcher.add(\"OPS_QUANTIFIERS\", [pattern])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "matches = matcher(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bought a smartphone\n",
      "buying apps\n"
     ]
    }
   ],
   "source": [
    "for match_id, start, end in matches:\n",
    "    matched_span = doc[start:end]\n",
    "    print(matched_span.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"OP\" can have one of four values:\n",
    "\n",
    "An \"!\" negates the token, so it's matched 0 times.\n",
    "\n",
    "A \"?\" makes the token optional, and matches it 0 or 1 times.\n",
    "\n",
    "A \"+\" matches a token 1 or more times.\n",
    "\n",
    "And finally, an \"*\" matches 0 or more times.\n",
    "\n",
    "Operators can make your patterns a lot more powerful, but they also add more complexity – so use them wisely.\n",
    "\n",
    "|Example|Description|\n",
    "|-|-|\n",
    "|{\"OP\": \"!\"}|Negation: match 0 times|\n",
    "|{\"OP\": \"?\"}|Optional: match 0 or 1 times|\n",
    "|{\"OP\": \"+\"}|Match 1 or more times|\n",
    "|{\"OP\": \"*\"}|Match 0 or more times|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TASK\n",
    "\n",
    "Let’s try spaCy’s rule-based Matcher. You’ll be using the example from the previous exercise and write a pattern that can match the phrase “iPhone X” in the text.\n",
    "\n",
    "1. Import the Matcher from spacy.matcher.\n",
    "2. Initialize it with the nlp object’s shared vocab.\n",
    "3. Create a pattern that matches the \"TEXT\" values of two tokens: \"iPhone\" and \"X\".\n",
    "4. Use the matcher.add method to add the pattern to the matcher.\n",
    "5. Call the matcher on the doc and store the result in the variable matches.\n",
    "6. Iterate over the matches and get the matched span from the start to the end index."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
