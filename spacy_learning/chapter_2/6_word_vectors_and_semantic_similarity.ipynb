{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Word vectors and semantic similarity\n",
    "\n",
    "In this lesson, you'll learn how to use spaCy to predict how similar documents, spans or tokens are to each other.\n",
    "\n",
    "You'll also learn how to use word vectors and how to take advantage of them in your NLP application."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "spaCy can compare two objects and predict how similar they are â€“ for example, documents, spans or single tokens.\n",
    "\n",
    "The Doc, Token and Span objects have a .similarity method that takes another object and returns a floating point number between 0 and 1, indicating how similar they are.\n",
    "\n",
    "One thing that's very important: In order to use similarity, you need a larger spaCy pipeline that has word vectors included.\n",
    "\n",
    "For example, the medium or large English pipeline â€“ but not the small one. So if you want to use vectors, always go with a pipeline that ends in \"md\" or \"lg\". You can find more details on this in the documentation.\n",
    "\n",
    "#### Comparing semantic similarity\n",
    "- spaCy can compare two objects and predict similarity\n",
    "- Doc.similarity(), Span.similarity() and Token.similarity()\n",
    "- Take another object and return a similarity score (0 to 1)\n",
    "\n",
    "Important: needs a pipeline that has word vectors included, for example:\n",
    "- âœ… en_core_web_md (medium)\n",
    "- âœ… en_core_web_lg (large)\n",
    "- ðŸš« NOT en_core_web_sm (small)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Here's an example.\n",
    "\n",
    "Let's say we want to find out whether two documents are similar.\n",
    "\n",
    "First, we load the medium English pipeline, \"en_core_web_md\".\n",
    "\n",
    "We can then create two doc objects and use the first doc's similarity method to compare it to the second.\n",
    "\n",
    "Here, a fairly high similarity score of 0.86 is predicted for \"I like fast food\" and \"I like pizza\".\n",
    "\n",
    "The same works for tokens.\n",
    "\n",
    "According to the word vectors, the tokens \"pizza\" and \"pasta\" are kind of similar, and receive a score of 0.7."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Similarity examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"en_core_web_md\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8698332283318978\n"
     ]
    }
   ],
   "source": [
    "doc1 = nlp(\"I like fast food\")\n",
    "doc2 = nlp(\"I like pizza\")\n",
    "\n",
    "print(doc1.similarity(doc2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.685019850730896\n"
     ]
    }
   ],
   "source": [
    "doc = nlp(\"I like pizza and pasta\")\n",
    "\n",
    "token1 = doc[2]\n",
    "token2 = doc[4]\n",
    "\n",
    "print(token1.similarity(token2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also use the similarity methods to compare different types of objects.\n",
    "\n",
    "For example, a document and a token.\n",
    "\n",
    "Here, the similarity score is pretty low and the two objects are considered fairly dissimilar.\n",
    "\n",
    "Here's another example comparing a span â€“ \"pizza and pasta\" â€“ to a document about McDonalds.\n",
    "\n",
    "The score returned here is 0.61, so it's determined to be kind of similar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.1821369691957915\n"
     ]
    }
   ],
   "source": [
    "# Compare a document with a token\n",
    "doc = nlp(\"I like pizza\")\n",
    "token = nlp(\"soap\")[0]\n",
    "\n",
    "print(doc.similarity(token))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.47190033157126826\n"
     ]
    }
   ],
   "source": [
    "# Compare a span with a document\n",
    "span = nlp(\"I like pizza and pasta\")[2:5]\n",
    "doc = nlp(\"McDonalds sells burgers\")\n",
    "\n",
    "print(span.similarity(doc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But how does spaCy do this under the hood?\n",
    "\n",
    "Similarity is determined using word vectors, multi-dimensional representations of meanings of words.\n",
    "\n",
    "You might have heard of Word2Vec, which is an algorithm that's often used to train word vectors from raw text.\n",
    "\n",
    "Vectors can be added to spaCy's pipelines.\n",
    "\n",
    "By default, the similarity returned by spaCy is the cosine similarity between two vectors â€“ but this can be adjusted if necessary.\n",
    "\n",
    "Vectors for objects consisting of several tokens, like the Doc and Span, default to the average of their token vectors.\n",
    "\n",
    "That's also why you usually get more value out of shorter phrases with fewer irrelevant words."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How does spaCy predict similarity?\n",
    "- Similarity is determined using word vectors\n",
    "- Multi-dimensional meaning representations of words\n",
    "- Generated using an algorithm like Word2Vec and lots of text\n",
    "- Can be added to spaCy's pipelines\n",
    "- Default: cosine similarity, but can be adjusted\n",
    "- Doc and Span vectors default to average of token vectors\n",
    "- Short phrases are better than long documents with many irrelevant words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To give you an idea of what those vectors look like, here's an example.\n",
    "\n",
    "1. First, we load the medium pipeline again, which ships with word vectors.\n",
    "\n",
    "2. Next, we can process a text and look up a token's vector using the .vector attribute.\n",
    "\n",
    "3. The result is a 300-dimensional vector of the word \"banana\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"en_core_web_md\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = nlp(\"I like banana\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.20778 , -2.4151  ,  0.36605 ,  2.0139  , -0.23752 , -3.1952  ,\n",
       "       -0.2952  ,  1.2272  , -3.4129  , -0.54969 ,  0.32634 , -1.0813  ,\n",
       "        0.55626 ,  1.5195  ,  0.97797 , -3.1816  , -0.37207 , -0.86093 ,\n",
       "        2.1509  , -4.0845  ,  0.035405,  3.5702  , -0.79413 , -1.7025  ,\n",
       "       -1.6371  , -3.198   , -1.9387  ,  0.91166 ,  0.85409 ,  1.8039  ,\n",
       "       -1.103   , -2.5274  ,  1.6365  , -0.82082 ,  1.0278  , -1.705   ,\n",
       "        1.5511  , -0.95633 , -1.4702  , -1.865   , -0.19324 , -0.49123 ,\n",
       "        2.2361  ,  2.2119  ,  3.6654  ,  1.7943  , -0.20601 ,  1.5483  ,\n",
       "       -1.3964  , -0.50819 ,  2.1288  , -2.332   ,  1.3539  , -2.1917  ,\n",
       "        1.8923  ,  0.28472 ,  0.54285 ,  1.2309  ,  0.26027 ,  1.9542  ,\n",
       "        1.1739  , -0.40348 ,  3.2028  ,  0.75381 , -2.7179  , -1.3587  ,\n",
       "       -1.1965  , -2.0923  ,  2.2855  , -0.3058  , -0.63174 ,  0.70083 ,\n",
       "        0.16899 ,  1.2325  ,  0.97006 , -0.23356 , -2.094   , -1.737   ,\n",
       "        3.6075  , -1.511   , -0.9135  ,  0.53878 ,  0.49268 ,  0.44751 ,\n",
       "        0.6315  ,  1.4963  ,  4.1725  ,  2.1961  , -1.2409  ,  0.4214  ,\n",
       "        2.9678  ,  1.841   ,  3.0133  , -4.4652  ,  0.96521 , -0.29787 ,\n",
       "        4.3386  , -1.2527  , -1.7734  , -3.5637  , -0.20035 , -3.3013  ,\n",
       "        0.99951 , -0.92888 , -0.94594 ,  1.5124  , -3.9385  ,  2.7935  ,\n",
       "       -3.1042  ,  3.3382  ,  0.54513 , -0.37663 ,  2.5151  ,  0.51468 ,\n",
       "       -0.88907 ,  1.011   ,  3.4705  , -3.6037  ,  1.3702  ,  2.3468  ,\n",
       "        1.6674  ,  1.3904  , -2.8112  ,  2.237   , -1.0344  , -0.57164 ,\n",
       "        1.0641  , -1.6919  ,  1.958   , -0.78305 ,  0.14741 ,  0.51083 ,\n",
       "        1.8278  , -0.69638 ,  0.90548 ,  0.62282 , -1.8315  , -2.8587  ,\n",
       "        0.48424 , -2.0527  , -0.53808 , -2.3472  ,  1.0354  , -1.8257  ,\n",
       "       -0.3892  , -0.24943 ,  0.8651  , -1.5195  ,  1.2166  , -2.698   ,\n",
       "       -0.96698 ,  2.2175  , -0.16089 , -0.49677 , -0.19646 ,  1.3284  ,\n",
       "        4.0824  ,  1.3919  ,  0.80669 , -1.0316  , -0.28056 , -1.8632  ,\n",
       "        0.47716 , -0.53628 ,  1.3853  , -2.1755  , -0.2354  ,  2.4933  ,\n",
       "       -0.87255 ,  1.4493  , -0.10778 , -0.44159 ,  1.3462  ,  4.4211  ,\n",
       "       -1.8385  ,  0.3985  ,  0.47637 , -0.60074 ,  3.3583  , -0.15006 ,\n",
       "       -0.40495 ,  2.7225  , -1.6297  ,  0.86797 , -4.1445  , -2.7793  ,\n",
       "        1.1535  , -0.011691,  0.9792  , -1.0141  ,  0.80134 ,  0.43642 ,\n",
       "        1.4337  ,  2.8927  ,  0.82871 , -1.1827  , -1.3838  ,  2.3903  ,\n",
       "       -0.89323 ,  1.1461  , -1.7435  ,  0.8654  , -0.27075 , -0.78698 ,\n",
       "        1.5631  , -0.5923  ,  0.098082, -0.26682 ,  1.6282  , -0.77495 ,\n",
       "        3.2552  ,  1.7964  , -1.4314  ,  1.2336  ,  2.3102  , -1.6328  ,\n",
       "        2.8366  , -0.71384 ,  0.43967 ,  1.5627  ,  3.079   , -0.922   ,\n",
       "       -0.43981 , -0.7659  ,  1.9362  , -2.2479  ,  1.041   ,  0.63206 ,\n",
       "        1.5855  ,  3.4097  , -2.9204  , -1.4751  , -0.59534 , -1.688   ,\n",
       "       -4.1362  ,  2.745   , -2.8515  ,  3.6509  , -0.66993 , -2.8794  ,\n",
       "        2.0733  ,  1.1779  , -2.0307  ,  2.595   , -0.12246 ,  1.5844  ,\n",
       "        1.1855  ,  0.022385, -2.2916  , -2.2684  , -2.7537  ,  0.34981 ,\n",
       "       -4.6243  , -0.96521 , -1.1435  , -2.8894  , -0.12619 ,  2.9577  ,\n",
       "       -1.7227  ,  0.24757 ,  1.2149  ,  3.5349  , -0.95802 ,  0.080346,\n",
       "       -1.6553  , -0.6734  ,  2.2918  , -1.8229  , -1.1336  ,  1.8884  ,\n",
       "        2.4789  , -0.66061 ,  2.0529  , -0.76687 ,  0.32362 , -2.2579  ,\n",
       "        0.91278 ,  0.36231 ,  0.61562 , -0.15396 , -0.42917 , -0.89848 ,\n",
       "        0.17298 , -0.76978 , -2.0222  , -1.7127  , -1.5632  ,  0.56631 ,\n",
       "       -1.354   ,  2.6261  ,  1.9156  , -1.5651  ,  1.8315  , -1.4257  ,\n",
       "       -1.6861  , -0.51953 ,  1.7635  , -0.50722 ,  1.388   , -1.1012  ],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc[2].vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Similarity depends on the application context\n",
    "\n",
    "Predicting similarity can be useful for many types of applications. For example, to recommend a user similar texts based on the ones they have read. It can also be helpful to flag duplicate content, like posts on an online platform.\n",
    "\n",
    "However, it's important to keep in mind that there's no objective definition of what's similar and what isn't. It always depends on the context and what your application needs to do.\n",
    "\n",
    "Here's an example: spaCy's default word vectors assign a very high similarity score to \"I like cats\" and \"I hate cats\". This makes sense, because both texts express sentiment about cats. But in a different application context, you might want to consider the phrases as very dissimilar, because they talk about opposite sentiments.\n",
    "\n",
    "- Useful for many applications: recommendation systems, flagging duplicates etc.\n",
    "- There's no objective definition of \"similarity\"\n",
    "- Depends on the context and what application needs to do"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9530094042245597\n"
     ]
    }
   ],
   "source": [
    "doc1 = nlp(\"I like cats\")\n",
    "doc2 = nlp(\"I hate cats\")\n",
    "\n",
    "print(doc1.similarity(doc2))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
